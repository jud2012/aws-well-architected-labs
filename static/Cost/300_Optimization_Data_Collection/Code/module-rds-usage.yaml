AWSTemplateFormatVersion: '2010-09-09'
Description: Retrieves RDS Metric data
Parameters:
  DatabaseName:
    Type: String
    Description: Name of the Athena database to be created to hold lambda information
    Default: optimization_data
  DestinationBucket:
    Type: String
    Description: Name of the S3 Bucket to be created to hold data information
    AllowedPattern: (?=^.{3,63}$)(?!^(\d+\.)+\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])\.)*([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])$)
  DestinationBucketARN:
    Type: String
    Description: ARN of the S3 Bucket that exists or needs to be created to hold rightsizing information
  MultiAccountRoleName:
    Type: String
    Description: Name of the IAM role deployed in all accounts which can retrieve AWS Data.
  CFDataName:
    Type: String
    Description: The name of what this cf is doing.
    Default: rds-usage
  DAYS:
    Type: Number
    Description: Number of days going back that you want to get data for
    Default: 1
  GlueRoleArn:
    Type: String
    Description: ARN of the IAM role deployed to use for glue.
  RolePrefix:
    Type: String
    Description: This prefix will be placed in front of all roles created. Note you may wish to add a dash at the end to make more readable
  LambdaAnalyticsARN:
    Type: String
    Description: Arn of lambda for Analytics
Outputs:
  AthenaSavedQuery:
    Description: This saved query will provide you a summary of your lambda data
    Value:
      Ref: AthenaQuery
  LambdaRoleARN:
    Description: Role for Lambda execution of lambda data.
    Value:
      Fn::GetAtt:
        - LambdaRole
        - Arn
  SQSUrl:
    Description: TaskQueue URL the account collector lambda
    Value: !Ref TaskQueue
Resources:
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${RolePrefix}Lambda-Role-${CFDataName}"
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
        Version: 2012-10-17
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AWSLambdaExecute
      Path: /
      Policies:
        - PolicyName: !Sub "Assume-Management-${CFDataName}-Account-Role"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action: "sts:AssumeRole"
                Resource: "*"
        - PolicyName: !Sub "${CFDataName}-Access"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                  - "s3:PutObject"
                  - "s3:ListBucket"
                  - "s3:GetBucketLocation"
                Resource: !Ref  DestinationBucketARN
              - Effect: "Allow"
                Action:
                  - "glue:StartCrawler"
                Resource: !Sub "arn:aws:glue:${AWS::Region}:${AWS::AccountId}:crawler/${Crawler}"
              - Effect: "Allow"
                Action:
                  - "cloudwatch:GetMetricStatistics"
                Resource: "*"  ## only * can be used as a Resource for this action
              - Effect: "Allow"
                Action:
                  - "sqs:ReceiveMessage"
                  - "sqs:DeleteMessage"
                  - "sqs:GetQueueAttributes"
                Resource: !Sub arn:aws:sqs:${AWS::Region}:${AWS::AccountId}:RDSUtilizationModule-TaskQueue
              - Effect: "Allow"
                Action:
                  - "rds:DescribeDBProxyTargetGroups"
                  - "rds:DescribeDBInstanceAutomatedBackups"
                  - "rds:DescribeDBEngineVersions"
                  - "rds:DescribeDBSubnetGroups"
                  - "rds:DescribeGlobalClusters"
                  - "rds:DescribeExportTasks"
                  - "rds:DescribePendingMaintenanceActions"
                  - "rds:DescribeEngineDefaultParameters"
                  - "rds:DescribeDBParameterGroups"
                  - "rds:DescribeDBClusterBacktracks"
                  - "rds:DescribeReservedDBInstancesOfferings"
                  - "rds:DescribeDBProxyTargets"
                  - "rds:DownloadDBLogFilePortion"
                  - "rds:DescribeDBInstances"
                  - "rds:DescribeSourceRegions"
                  - "rds:DescribeEngineDefaultClusterParameters"
                  - "rds:DescribeDBProxies"
                  - "rds:DescribeDBParameters"
                  - "rds:DescribeEventCategories"
                  - "rds:DescribeDBProxyEndpoints"
                  - "rds:DescribeEvents"
                  - "rds:DescribeDBClusterSnapshotAttributes"
                  - "rds:DescribeDBClusterParameters"
                  - "rds:DescribeEventSubscriptions"
                  - "rds:DescribeDBSnapshots"
                  - "rds:DescribeDBLogFiles"
                  - "rds:DescribeDBSecurityGroups"
                  - "rds:DescribeDBSnapshotAttributes"
                  - "rds:DescribeReservedDBInstances"
                  - "rds:ListTagsForResource"
                  - "rds:DescribeValidDBInstanceModifications"
                  - "rds:DescribeDBClusterSnapshots"
                  - "rds:DescribeOrderableDBInstanceOptions"
                  - "rds:DescribeOptionGroupOptions"
                  - "rds:DescribeDBClusterEndpoints"
                  - "rds:DescribeCertificates"
                  - "rds:DescribeDBClusters"
                  - "rds:DescribeAccountAttributes"
                  - "rds:DescribeOptionGroups"
                  - "rds:DescribeDBClusterParameterGroups"
                  - "ec2:DescribeRegions"
                Resource: !Sub "arn:aws:rds:${AWS::Region}:${AWS::AccountId}:*"
              - Effect: "Allow"
                Action:
                  - "ec2:DescribeRegions"
                Resource: "*"  ## only * can be used as a Resource for this action                
  LambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub
          - '${CFDataName}-Lambda-Function-${Id}'
          - Id: !Select [0, !Split ['-', !Ref AWS::StackName]]
      Description: !Sub "LambdaFunction to retrieve ${CFDataName}"
      Runtime: python3.8
      Code:
        ZipFile: |
          import boto3
          from botocore.exceptions import ClientError
          import os
          from boto3.s3.transfer import S3Transfer
          from datetime import datetime, timedelta, date
          import json
          from re import sub
          import logging

          #Environment Variables
          bucket = os.environ['S3_BUCKET']
          prefix = os.environ['PREFIX']
          crawler = os.environ["CRAWLER_NAME"]

          functions = {'rds':
                          {'regional' : 1,
                          'api' : 'rds',
                          'functions' : [
                              {'name' : 'get_rds_stats',
                              'output_path' : 'rds-usage-data',
                              'output_file_name' : 'rds_stats.json' }
                          ]
                          }
                      }

          METRICS_FOR_VOLUMES = [
              'FreeableMemory',
              'CPUUtilization',
              'NetworkReceiveThroughput',
              'NetworkTransmitThroughput',
              'ReadIOPS',
              'WriteIOPS',
              'FreeStorageSpace'
              ]

          TAGS_TO_RETRIEVE = [
              'Environment',
              'Schedule',
              'Application',
              'Project',
          ]


          now = datetime.utcnow()
          DAYS = int(os.environ['DAYS'])
          past = now - timedelta(days=DAYS)
          today = datetime.today().strftime('%Y-%m-%d')
          tod = date.today()
          year = tod.year
          month = tod.strftime('%m')
          future = now# + timedelta(minutes=10)
          period = 3600

          def format_rds(rds):

              if len(rds["Attachments"]) == 0:
                  instance_id = "unattached"
                  device = ""
              else:
                  instance_id = rds["Attachments"][0]["InstanceId"] if rds["Attachments"][0]["InstanceId"] else ""
                  device = rds["Attachments"][0]["Device"] if rds["Attachments"][0]["Device"] else ""

              output = {
                  "az": rds["AvailabilityZone"],
                  "instanceId": instance_id,
                  "rdsId": rds["VolumeId"],
                  "device": device,
                  "rdsType": rds["VolumeType"],
                  "sizeGB": rds["Size"],
                  "IOPS": rds["Iops"],
                  "rdsCreateTime": rds["CreateTime"]
              }

              tags = {}
              filtered_tags = {}
              if "Tags" in rds:
                  for tag in rds["Tags"]:
                      tags[sub('[^a-zA-Z0-9-_ *.]', '', tag["Key"].replace(",", " "))] = sub('[^a-zA-Z0-9-_ *.]', '', tag["Value"].replace(",", " "))
              for tags_required in TAGS_TO_RETRIEVE:
                  filtered_tags[tags_required] = tags[tags_required] if tags_required in tags else ''

              output["tags"] = filtered_tags

              return output


          def get_rds_ids(rds_client):
              rds_inventory = []
              rds_information = rds_client.describe_rds(Filters=[{'Name': 'status','Values':['in-use','available']}])
              if not rds_information or not rds_information['Volumes']:
                  rds_information = []
              else:
                  rds_information = rds_information['Volumes']

              for rds in rds_information:
                  rds_inventory.append(format_rds(rds))

              return rds_inventory

  
          def store_data_to_s3(data, region, service, path, resource_type, resource_value, filename, accountID, payer_id):
              local_file = f"/tmp/{region}-{filename}"
              with open(local_file, 'w') as f:
                  json.dump(data, f, default=str)
                  f.write('\n')
              if os.path.getsize(local_file) == 0:
                  print(f"No data in file for {path}")
                  return
              key = f"{prefix}/{path}/payer_id={payer_id}/accountid={accountID}/region={region}/{resource_type}={resource_value}/year={year}/month={month}/{today}-{filename}"
              s3client = boto3.client('s3')
              print("Uploading file %s to %s/%s" %(local_file, bucket, key))
              S3Transfer(s3client).upload_file(local_file, bucket, key, extra_args={'ACL': 'bucket-owner-full-control'})
              print('file upload successful')

          def start_crawler():
              glue_client = boto3.client('glue')
              try:
                  glue_client.start_crawler(Name=crawler)
                  print("crawler started")
              except Exception as e:
                  # Send some context about this error to Lambda Logs
                  print("%s" % e)

          def get_rds_stats(cwclient, client, s3client, region, service, path, filename, accountID, payer_id):
              datapoints = {}
              data = {}
              rds_inventory = client.describe_db_instances()

              for rds in rds_inventory['DBInstances']:
                  
                  for metric in METRICS_FOR_VOLUMES:
                      results = cwclient.get_metric_statistics(StartTime=past, EndTime=future, MetricName=metric,
                      Namespace='AWS/RDS',Statistics=['Average','Maximum','Minimum'],Period=period,
                      Dimensions=[{"Name": "DBInstanceIdentifier", "Value": rds["DBInstanceIdentifier"]}])
                      datapoints.update({metric:results['Datapoints']})
                  rds["Datapoints"] = datapoints
                  store_data_to_s3(rds, region, service, path, 'rds_id', rds['DBInstanceIdentifier'], filename, accountID, payer_id)
          
          def assume_role(account_id, service, region):
            role_name = os.environ['ROLENAME']
            role_arn = f"arn:aws:iam::{account_id}:role/{role_name}" 
            sts_client = boto3.client('sts')
            
            try:
                assumedRoleObject = sts_client.assume_role(
                    RoleArn=role_arn,
                    RoleSessionName="AssumeRoleRoot"
                    )
                
                credentials = assumedRoleObject['Credentials']
                client = boto3.client(
                    service,
                    aws_access_key_id=credentials['AccessKeyId'],
                    aws_secret_access_key=credentials['SecretAccessKey'],
                    aws_session_token=credentials['SessionToken'],
                    region_name = region
                )
                return client

            except ClientError as e:
                logging.warning(f"Unexpected error Account {account_id}: {e}")
                return None


          def lambda_handler(event, context):
              try:
                  if 'Records' not in event: 
                      raise Exception("Please do not trigger this Lambda manually. Find an Accounts-Collector-Function-OptimizationDataCollectionStack Lambda  and Trigger from there.")
                  
                  for record in event['Records']:
                      body = json.loads(record["body"])
                      account_id = body["account_id"]
                      payer_id = body["payer_id"]
                      print(account_id)
                      s3client = boto3.client('s3')
                      regions = get_supported_regions('ec2')
                      
                      
                      for service in functions.keys():
                          if functions[service]['regional']:
                              for region in regions:
                                  client = assume_role(account_id, functions[service]['api'], region['RegionName'])
                                  for f in functions[service]['functions']:
                                      cw_client = assume_role(account_id, 'cloudwatch', region['RegionName'])
                                      try:
                                          data = globals()[f['name']](cw_client, client, s3client, region['RegionName'], service, f['output_path'], f['output_file_name'], account_id, payer_id)
                                      except Exception as e:
                                          # Send some context about this error to Lambda Logs
                                          logging.warning(e)
                          else:
                              client=boto3.client(service)
                              for f in functions[service]['functions']:
                                  cw_client = boto3.client('cloudwatch', region_name = 'us-east-1')
                                  try:
                                      data = globals()[f['name']](cw_client, client, s3client, 'us-east-1', service, f['output_path'], f['output_file_name'], account_id)
                                  except Exception as e:
                                      # Send some context about this error to Lambda Logs
                                      logging.warning(e)
                      start_crawler()
                      return "Successful"
              except Exception as e:
                  # Send some context about this error to Lambda Logs
                  logging.warning(e)



          def get_supported_regions(service):
              response = []
              if service == 'ec2':
                  ec2_c = boto3.client('ec2')
                  response = ec2_c.describe_regions()
              return response['Regions'] if response['Regions'] else []


      Handler: 'index.lambda_handler'
      MemorySize: 2688
      Timeout: 300
      Role:
        Fn::GetAtt:
          - LambdaRole
          - Arn
      Environment:
        Variables:
          S3_BUCKET:
            !Ref DestinationBucket
          ACCOUNT_ID: AWS::AccountId
          DAYS:
            !Ref DAYS
          CRAWLER_NAME: !Ref Crawler
          PREFIX: !Ref CFDataName
          ROLENAME: !Ref MultiAccountRoleName
  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Sub
          - '${CFDataName}-Crawler-${Id}'
          - Id: !Select [0, !Split ['-', !Ref AWS::StackName]]
      Role: !Ref GlueRoleArn
      DatabaseName: !Ref DatabaseName
      Targets:
        S3Targets:
          - Path: !Sub "s3://${DestinationBucket}/${CFDataName}/${CFDataName}-data/"
      Configuration: "{\"Version\":1.0,\"CrawlerOutput\":{\"Partitions\":{\"AddOrUpdateBehavior\":\"InheritFromTable\"}}}"
  AthenaQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref DatabaseName
      Description: Provides a summary view of the lambda data
      Name: !Sub ${CFDataName}-summary-view
      QueryString: !Sub |
        SELECT dbinstanceidentifier, 'memory' as Metric, memory.timestamp, memory.maximum
        FROM ${DatabaseName}.${CFDataName}
        cross join unnest(datapoints.FreeableMemory) as t(memory)
        union
        SELECT dbinstanceidentifier, 'cpu' as Metric, cpu.timestamp, cpu.maximum
        FROM ${DatabaseName}.${CFDataName}
        cross join unnest(datapoints.CPUUtilization) as t(cpu)
        union
        SELECT dbinstanceidentifier, 'NetworkReceiveThroughput' as Metric, NetworkReceiveThroughput.timestamp, NetworkReceiveThroughput.maximum
        FROM ${DatabaseName}.${CFDataName}
        cross join unnest(datapoints.NetworkReceiveThroughput) as t(NetworkReceiveThroughput)
        union
        SELECT dbinstanceidentifier, 'NetworkTransmitThroughput' as Metric, NetworkTransmitThroughput.timestamp, NetworkTransmitThroughput.maximum
        FROM ${DatabaseName}.${CFDataName}
        cross join unnest(datapoints.CPUUtilization) as t(NetworkTransmitThroughput)
        union
        SELECT dbinstanceidentifier, 'ReadIOPS' as Metric, ReadIOPS.timestamp, ReadIOPS.maximum
        FROM ${DatabaseName}.${CFDataName}
        cross join unnest(datapoints.ReadIOPS) as t(ReadIOPS)
        union
        SELECT dbinstanceidentifier, 'WriteIOPS' as Metric, WriteIOPS.timestamp, WriteIOPS.maximum
        FROM ${DatabaseName}.${CFDataName}
        cross join unnest(datapoints.WriteIOPS) as t(WriteIOPS)
        union
        SELECT dbinstanceidentifier, 'FreeStorageSpace' as Metric, FreeStorageSpace.timestamp, FreeStorageSpace.maximum
        FROM ${DatabaseName}.${CFDataName}
        cross join unnest(datapoints.FreeStorageSpace) as t(FreeStorageSpace)
  EventPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !GetAtt LambdaFunction.Arn
      Action: lambda:InvokeFunction
      Principal: sqs.amazonaws.com
      SourceAccount: !Ref 'AWS::AccountId'
      SourceArn: !GetAtt TaskQueue.Arn
  TaskQueue: 
    Type: AWS::SQS::Queue
    Properties: 
      VisibilityTimeout: 300
      ReceiveMessageWaitTimeSeconds: 20
      DelaySeconds: 2
      KmsMasterKeyId: "alias/aws/sqs"
      QueueName: 'RDSUtilizationModule-TaskQueue'
  EventSourceMapping:
    DependsOn:
      - EventPermission
    Type: AWS::Lambda::EventSourceMapping
    Properties:
      EventSourceArn: !GetAtt TaskQueue.Arn
      FunctionName: !GetAtt LambdaFunction.Arn
  LambdaAnalyticsExecutor:
    Type: Custom::LambdaAnalyticsExecutor
    Properties:
      ServiceToken: !Ref LambdaAnalyticsARN
      Name: !Ref CFDataName
  GravitonMappingTable:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref DatabaseName
      Description: Provides a summary view of the lambda data
      Name: graviton_mapping
      QueryString: !Sub |
        CREATE EXTERNAL TABLE `rds_graviton_mapping`(
          `dbtype` varchar(255), 
          `databaseengine` varchar(255), 
          `instancetype` varchar(255), 
          `graviton_instancetype` varchar(255))
          ROW FORMAT DELIMITED 
          FIELDS TERMINATED BY ',' 
        STORED AS INPUTFORMAT 
          'org.apache.hadoop.mapred.TextInputFormat' 
        OUTPUTFORMAT 
          'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
        LOCATION
          's3://${DestinationBucket}/pricing/graviton'
        TBLPROPERTIES (
          'classification'='csv', 
          'transient_lastDdlTime'='1666191659')

  GravitonAthenaQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref DatabaseName
      Description: Provides a view of modernization opportunities for Graviton
      Name: !Sub ${CFDataName}-rds-graviton
      QueryString: |
        WITH 
        raw_data AS (
          SELECT dbinstanceidentifier,
            dbinstanceclass,
            concat(
              split_part(dbinstanceclass, '.', 1),
              '.',
              split_part(dbinstanceclass, '.', 2)
            ) instance_family,
            availabilityzone,
            rtrim(availabilityzone, 'abcdef') region,
            (CASE
                WHEN (multiaz = false) THEN 'Single-AZ'
                WHEN (multiaz = true) THEN 'Multi-AZ' ELSE 'Error check deploymentoption'
              END
            ) deploymentoption,
            (CASE
                WHEN (engine LIKE 'aurora%') THEN 'Aurora' ELSE 'AmazonRDS'
              END
            ) rds_type,
            engine,
            (CASE
                WHEN (engine LIKE '%postgres%') THEN 'PostgreSQL'
                WHEN (engine LIKE '%mysql%') THEN 'MySQL'
                WHEN (engine LIKE '%mariadb%') THEN 'MariaDB'
                WHEN (engine LIKE '%oracle%') THEN 'Oracle'
                WHEN (engine LIKE '%docdb%') THEN 'DocDB'
                WHEN (engine LIKE '%sqlserver%') THEN 'SQL Server'
                WHEN (engine LIKE 'aurora 5.6.10a') THEN 'aurora 5.6.10a'
                WHEN (engine LIKE '%neptune%') THEN 'Neptune' ELSE 'Check DB Engine'
              END) db_engine,
            (CASE
                WHEN (engineversion LIKE '%aurora%') THEN concat(
                  split_part(substr(engineversion, 18, 6), '.', 1),
                  '.',
                  split_part(substr(engineversion, 18, 6), '.', 2),
                  '.',
                  split_part(substr(engineversion, 18, 6), '.', 3)
                ) ELSE engineversion
              END) engineversion,
            (CASE
                WHEN (engineversion LIKE '%aurora%') THEN split_part(substr(engineversion, 18, 6), '.', 1) ELSE split_part(engineversion, '.', 1)
              END) major,
            (CASE
                WHEN (engineversion LIKE '%aurora%') THEN split_part(substr(engineversion, 18, 6), '.', 2) ELSE split_part(engineversion, '.', 2)
              END) minor,
            (CASE
                WHEN (engineversion LIKE '%aurora%') THEN split_part(substr(engineversion, 18, 6), '.', 3) ELSE split_part(engineversion, '.', 3)
              END) fix,
            accountid,
            year,
            month,
            CAST("concat"("year", '-', "month", '-01') AS date) "billing_period"
          FROM rds_usage_data),
        rds_pricing AS (
          SELECT (CASE
                WHEN (databaseengine LIKE 'Aurora%') THEN 'Aurora'
                WHEN (databaseengine = 'Any') THEN 'Any' ELSE 'AmazonRDS'
              END
            ) rds_type,
            (CASE
                WHEN (databaseengine LIKE 'Aurora%') THEN split_part(databaseengine, ' ', 2) ELSE databaseengine
              END
            ) db_engine,
            deploymentoption,
            location,
            instancetype,
            (CASE
                WHEN (priceperunit = '') THEN 0E0 ELSE CAST(priceperunit AS decimal(18,2))
              END
            ) priceperunit,
            unit
          FROM optimization_data.pricing_rds
          WHERE (((locationtype = 'AWS Region')AND (purchaseoption = ''))AND (productfamily = 'Database Instance'))),
        graviton_mapping AS (
          SELECT raw_data.dbinstanceidentifier,
            raw_data.rds_type,
            raw_data.db_engine,
            raw_data.dbinstanceclass,
            (CASE
                WHEN (raw_data.dbinstanceclass LIKE '%g.%') THEN 'Already Graviton'
                WHEN (raw_data.dbinstanceclass LIKE '%gd.%') THEN 'Already Graviton'
                WHEN (raw_data.dbinstanceclass LIKE '%serverless%') THEN 'Ineligible' ELSE (
                  CASE
                    WHEN (raw_data.rds_type = 'AmazonRDS') THEN (
                      CASE
                        WHEN (raw_data.db_engine = 'MySQL') THEN (
                          CASE
                            WHEN (CAST(raw_data.major AS integer) > 8) THEN 'Eligible'
                            WHEN (
                              (CAST(raw_data.major AS integer) = 8)
                              AND (CAST(raw_data.minor AS integer) > 0)
                            ) THEN 'Eligible'
                            WHEN (
                              (
                                (CAST(raw_data.major AS integer) = 8)
                                AND (CAST(raw_data.minor AS integer) = 0)
                              )
                              AND (CAST(raw_data.fix AS integer) >= 17)
                            ) THEN 'Eligible' ELSE 'Requires Update'
                          END
                        )
                        WHEN (raw_data.db_engine = 'PostgreSQL') THEN (
                          CASE
                            WHEN (CAST(raw_data.major AS integer) > 12) THEN 'Eligible'
                            WHEN (
                              (CAST(raw_data.major AS integer) = 12)
                              AND (CAST(raw_data.minor AS integer) >= 3)
                            ) THEN 'Eligible' ELSE 'Requires Update'
                          END
                        )
                        WHEN (raw_data.db_engine = 'MariaDB') THEN (
                          CASE
                            WHEN (CAST(raw_data.major AS integer) > 10) THEN 'Eligible'
                            WHEN (
                              (CAST(raw_data.major AS integer) = 10)
                              AND (CAST(raw_data.minor AS integer) > 4)
                            ) THEN 'Eligible'
                            WHEN (
                              (
                                (CAST(raw_data.major AS integer) = 10)
                                AND (CAST(raw_data.minor AS integer) = 4)
                              )
                              AND (CAST(raw_data.fix AS integer) >= 13)
                            ) THEN 'Eligible' ELSE 'Requires Update'
                          END
                        ) ELSE 'Ineligible'
                      END
                    )
                    WHEN (raw_data.rds_type = 'Aurora') THEN (
                      CASE
                        WHEN (raw_data.db_engine = 'MySQL') THEN (
                          CASE
                            WHEN (CAST(raw_data.major AS integer) > 2) THEN 'Eligible'
                            WHEN (
                              (CAST(raw_data.major AS integer) = 2)
                              AND (CAST(raw_data.minor AS integer) > 9)
                            ) THEN 'Eligible'
                            WHEN (
                              (
                                (CAST(raw_data.major AS integer) = 2)
                                AND (CAST(raw_data.minor AS integer) = 9)
                              )
                              AND (CAST(raw_data.fix AS integer) >= 2)
                            ) THEN 'Eligible' ELSE 'Ineligible'
                          END
                        )
                        WHEN (raw_data.db_engine = 'PostgreSQL') THEN (
                          CASE
                            WHEN (CAST(raw_data.major AS integer) > 12) THEN 'Eligible'
                            WHEN (
                              (CAST(raw_data.major AS integer) = 12)
                              AND (CAST(raw_data.minor AS integer) >= 4)
                            ) THEN 'Eligible' ELSE 'Ineligible'
                          END
                        ) ELSE 'Ineligible'
                      END
                    )
                  END
                )
              END
            ) graviton_eligible,
            rds_graviton_mapping.graviton_instancetype,
            raw_data.region,
            pricing_region_names.regionname,
            raw_data.deploymentoption,
            raw_data.engineversion,
            raw_data.accountid,
            raw_data.year,
            raw_data.month, 
            raw_data.billing_period
          FROM (
              (
                raw_data
                LEFT JOIN optimization_data.pricing_region_names ON (raw_data.region = pricing_region_names.region)
              )
              LEFT JOIN optimization_data.rds_graviton_mapping ON (
                (
                  (raw_data.rds_type = rds_graviton_mapping.dbtype)
                  AND (
                    raw_data.db_engine = rds_graviton_mapping.databaseengine
                  )
                )
                AND (
                  raw_data.dbinstanceclass = rds_graviton_mapping.instancetype
                )
              )
            )
        )
        SELECT graviton_mapping.*,
          PL1.priceperunit existing_unit_price,
          (PL1.priceperunit * 720) existing_monthly_price,
          PL2.priceperunit graviton_unit_price,
          (PL2.priceperunit * 720) graviton_montlhy_price,
          round(
            (
              (PL1.priceperunit * 720) - (PL2.priceperunit * 720)
            ),
            2
          ) monthly_savings,
          round(
            (
              (
                (PL1.priceperunit * 720) - (PL2.priceperunit * 720)
              ) * 12
            ),
            2
          ) estimated_annual_savings,
          round((1 - (PL2.priceperunit / PL1.priceperunit)), 2) percentage_savings
        FROM (
            (
              graviton_mapping
              LEFT JOIN rds_pricing PL1 ON (
                (
                  (
                    (
                      (graviton_mapping.rds_type = PL1.rds_type)
                      AND (graviton_mapping.db_engine = PL1.db_engine)
                    )
                    AND (
                      graviton_mapping.deploymentoption = PL1.deploymentoption
                    )
                  )
                  AND (graviton_mapping.regionname = PL1.location)
                )
                AND (
                  graviton_mapping.dbinstanceclass = PL1.instancetype
                )
              )
            )
            LEFT JOIN rds_pricing PL2 ON (
              (
                (
                  (
                    (
                      graviton_mapping.graviton_instancetype = PL2.instancetype
                    )
                    AND (graviton_mapping.regionname = PL2.location)
                  )
                  AND (graviton_mapping.rds_type = PL2.rds_type)
                )
                AND (graviton_mapping.db_engine = PL2.db_engine)
              )
              AND (
                graviton_mapping.deploymentoption = PL2.deploymentoption
              )
            )
          )
